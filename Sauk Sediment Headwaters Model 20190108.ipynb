{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sauk-Suiattle Sediment Model\n",
    "\n",
    "This data is compiled to digitally observe the Skagit Watershed, powered by HydroShare. <br />\n",
    "<img src= \"http://www.sauk-suiattle.com/images/Elliott.jpg\"\n",
    "style=\"float:left;width:175px;padding:20px\">   \n",
    "<br />\n",
    "This is a beta version of a computational network-based sediment model was developed in order to connect processes of sediment supply on hillslopes, routing in streams, and deposition in reservoirs. The sediment model is developed in a framework called Landlab and driven by a physically-based, distributed hydrology model called DHSVM. The coupled sediment-hydrology model is designed to integrate relevant temporal and spatial scales of hillslope geomorphology, hydroclimatology and river network processes along with answering questions that are relevant to engineering application. The coupled model framework is designed to be applicable in other global watersheds, and could be useful for predicting sediment budgets particularly in the face of environmental and land use/land cover changes. \n",
    "<br /> <img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\" style=\"float:right;width:120px;padding:20px\">  \n",
    "<br />\n",
    "#### A Watershed Dynamics Model by the Watershed Dynamics Research Group in the Civil and Environmental Engineering Department at the University of Washington "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Erkan To DO - -  add citations to Claire's AGU presentations and DOE report, Landlab paper, DHSVM-PNNL repo, others?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run this notebook:\n",
    "\n",
    "Click in each shaded cell below and \"shift + enter\" to run each code block. Alternatively, you can run groups of cells by clicking \"Cell\" on the menu above and selecting your run options from the pull-down menu. This is also where you can clear outputs from previous runs.\n",
    "\n",
    "If an error occurs, click on *Kernal* and *Restart and Clear Outputs* in the menu above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Sediment discharge, its frequency and magnitude, and the size distribution of sediment load vary in space and time in a river. Sediment input into channels can be dominated by a single source such as a stratovolcano (e.g., Glacier Peak), or can be from distributed sources on hillslopes and zero-order basins, as well as from the banks of the channel itself through bank erosion.  Sediment is either transported within the flow as suspended load or transported along the riverbed as bedload. Suspended sediment includes sediments that are transported in suspension as a result of flow shear stress. It is composed of wash load (mostly clay and silt that is more or less continually in suspension) and suspension from bedload during higher flow; bed material in suspension includes coarse silt and sand that may move as bed load during lower flow. Bedload is generally assumed to represent 10% (or more generally in the 5% to 20% range) of the total sediment load. This percentage also depends on the location in the channel network and the type of watershed soil type. The suspended sediment load varies with discharge, often increasing nonlinearly as discharge rises because the greater turbulence associated with higher discharge allows a greater load of sediment to be held in suspension. \n",
    "\n",
    "<br /> <img src=\"https://www.hydroshare.org/resource/d5f1f96849554e03aa98bed5aff36cf7/data/contents/SedimentConcentrationbyVolume_Jakob_Jordon_2001.JPG\" style=\"float:center;width:700px;padding:40px\">\n",
    "\n",
    "Figure 1. Classification of sediment flow processes with respect to sediment concentration, velocity, and process domains (modified from Jakob and Jordan, 2001).\n",
    "\n",
    "Two related questions of interest for predicting erosion rates and sediment yields in the Puget Sound region are:  \n",
    "\n",
    "1)\tDo rivers that drain stratovolcanos and retreating glaciers exhibit larger erosion rates, and how modern and long-term erosion rates compare? \n",
    "\n",
    "2)\tCan a single suspendend sediment transport relationship predict loads across Puget Sound basins?\n",
    "\n",
    "In stratovolcanoes in high Alpine areas like Glacier Peak, a large portion of the loose pyroclastic deposits (clastic rock formed by volcanic explosion or ejected from a volcanic vent) are eroded by glaciers. Currently there are over a dozen glaciers on the sides of this volcano. In the model developed in this Notebook, we highlight network sediment results from seven glaciers; five in the Headwater Suiattle River (HUC 171100060201) including Dusty, Chocolate, Cool, Suiattle, and Honeycomb Glaciers and two in the Miners Creek Suiattle River (HUC 171100060202) including Vista and Ermine Glaciers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Install Python packages and libraries \n",
    "The CUAHSI JupyterHub server provides many Python packages and libraries, but to add additional libraries to your personal user space, use the cell below.  To request an Installation to the server, visit https://github.com/hydroshare/hydroshare-jupyterhub, create a New Issue, and add the label 'Installation Request'. Uncomment the lines below to install the library. Python2 and Python3 kernels are both available. If you are a new Landlab user on Hydroshare you are advised to run the code block below in your first run of this tutorial, then comment them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import modules and define functions\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.stats\n",
    "\n",
    "#Import utilities for importing and exporting to HydroShare\n",
    "from utilities import hydroshare\n",
    "import ogh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = ChristinaBandaragoda\n",
      "   HS_RES_ID = 87dc5742cf164126a11ff45c3307fd9d\n",
      "   HS_RES_TYPE = compositeresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => ChristinaBandaragoda\n",
      "\n",
      "The hs_utils library requires a secure connection to your HydroShare account.\n",
      "Enter the HydroShare password for user 'ChristinaBandaragoda': ········\n",
      "Successfully established a connection with HydroShare\n",
      "Data will be loaded from and saved to:/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents\n"
     ]
    }
   ],
   "source": [
    "hs=hydroshare.hydroshare()\n",
    "homedir = ogh.mapContentFolder(str(os.environ[\"HS_RES_ID\"]))\n",
    "print('Data will be loaded from and saved to:'+homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run following cells to import extract and copy DHSVM temperature model Outflow.Only files on HydroShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hs.getResourceFromHydroShare('f16bdb504c6a4fb39e80ff1070a86704')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gunzip /home/jovyan/work/notebooks/data/f16bdb504c6a4fb39e80ff1070a86704/f16bdb504c6a4fb39e80ff1070a86704/data/contents/*/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!cp /home/jovyan/work/notebooks/data/f16bdb504c6a4fb39e80ff1070a86704/f16bdb504c6a4fb39e80ff1070a86704/data/contents/Sauk_1969-2001_biasLivneh_WRF_TH8-WhiteTH15_mu12.8_his/Outflow.Only ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DHSVM model output  - flow at each stream node\n",
    "streamflow_input='Outflow.Only'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Import DHSVM network files and modeled streamflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIS file developed from https://github.com/pnnl/DHSVM-PNNL/tree/master/CreateStreamNetwork_PythonV\n",
    "# Note for Zhuoron: This code needs it's own citation and reference\n",
    "network_input = os.path.join(homedir, 'streamfile_2km2_sauk.txt')\n",
    "network_input ='streamfile_2km2_sauk.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/notebooks/data/d5f1f96849554e03aa98bed5aff36cf7/d5f1f96849554e03aa98bed5aff36cf7/data/contents\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set Model Parameters \n",
    "\n",
    "#### Universal Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=9.81                     # acceleration of gravity (m/s2)\n",
    "rho_w=1000                 # density of water (kg/m3)\n",
    "v=10**-6                   # kinematic viscosity of water (m2/s)\n",
    "sg=2.65                    # specific gravity \n",
    "bd_fine=1.13                # bulk density of fine sediments in reservoir \n",
    "bd_coarse=1.71              # bulk density of coarse sediments in reservoir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hydraulic geometry parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hydraulic geometry (HG) parameters (note: a is constant, b is exponent)\n",
    "ref_stream= 178 # DHSVM stream number where observations are collected that is used to scale other streams. For Elwha, this is the Lake Mills stream.\n",
    "\n",
    "# stream width @ ref stream\n",
    "w_const_ref=41.6\n",
    "## stream depth HG parameters @ ref stream\n",
    "a_d_ref=0.24\n",
    "b_d_ref=0.41\n",
    "### stream velocity HG parameters @ ref stream\n",
    "a_u_ref=0.10\n",
    "b_u_ref=0.58\n",
    "\n",
    "## stream width HG upstream exponent\n",
    "exp_w_us=0.5\n",
    "## stream depth HG upstream exponent\n",
    "exp_d_us=0.4\n",
    "## stream velocity HG upstream exponent\n",
    "exp_u_us=0.1\n",
    "\n",
    "# roughness parameterization\n",
    "a_n=1.08\n",
    "b_n=-0.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomorphology parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drate=0.2/365.25 # denudation rate [mm/day]- based on computations/literature\n",
    "beta_mw=10*365.25 # lag time [days] between mass wasting events- exponenetial distribution parameter\n",
    "init_depth=1 # Set initial depth of sediment in channels [m] \n",
    "abrasion_alpha=0.027\n",
    "abrasion_alpha_Fg=0.027\n",
    "mw_pcnt_g=0.6 # percent of gravel in mass wasting deposit\n",
    "mw_pcnt_s=1-mw_pcnt_g # percent of sand in mass wasting deposit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bedload sediment transport parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sediment transport parameterization per Wilcock and Crowe, 2003\n",
    "# Channel grain size properties at Lake mills gage stream\n",
    "Ch_Fs_LM=0.37\n",
    "Ch_Fg_LM=1-Ch_Fs_LM\n",
    "d90ch_LM=0.0275 # m -  d90 of channel bed= 27.5 mm=0.0275 m\n",
    "dsandch_LM=0.00093 # m- d_sand of channel bed= 0.93 mm = 0.00093 m\n",
    "dgravelch_LM=0.0132 # m- d_gravel of channel bed= 13.2 mm = 0.0132 m\n",
    "dmeanch_LM=0.0125 # m- d_mean of channel bed= 12.5 mm = 0.0125 m\n",
    "\n",
    "# Wilcock- Crowe Equation Parameters\n",
    "A_gravel=14\n",
    "chi_gravel=0.894\n",
    "exp_gravel=0.5\n",
    "phi_prime_gravel=1.35\n",
    "\n",
    "A_sand=14\n",
    "chi_sand=0.894\n",
    "exp_sand=0.5\n",
    "phi_prime_sand=1.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suspended sediment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suspended Sediment Equation Parameters\n",
    "a_s=1.17*10**-4 # regression coefficient from Curran et al., 2009\n",
    "b_s_c=3        # regression coefficient for SS concentration from Curran et al., 2009\n",
    "b_s_l=4        # regression coefficient for SS load from Curran et al., 2009\n",
    "cf_s=1.07      # log-regression correction factor from Curran et al., 2009\n",
    "K_ss=0.0864       # unit conversion factor from Curran et al., 2009  \n",
    "\n",
    "\n",
    "# Network suspended sediment parametrization\n",
    "# Key references: Patil et al., 2012\n",
    "# for sand equations\n",
    "c0_ss=1.1038\n",
    "c1_ss=2.6626\n",
    "c2_ss=5.6497\n",
    "c3_ss=0.3822\n",
    "c4_ss=-0.6174\n",
    "c5_ss=0.1315\n",
    "c6_ss=-0.0091\n",
    "\n",
    "# for silt equations\n",
    "tau_c_fines=0.015*(bd_fine-1)\n",
    "a_w_m=0.08\n",
    "n_w_m=1.65\n",
    "b_w_m=3.5\n",
    "m_w_m=1.88\n",
    "c1_m=0.15\n",
    "c2_m=b_w_m/((2*m_w_m-1)**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Code Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array,value):\n",
    "    val = (np.abs(array-value)).argmin()\n",
    "    return array[val]\n",
    "\n",
    "# Upload observed data\n",
    "def create_q_obs_df(file_name, drainage_area):\n",
    "    q=pd.read_excel(file_name, sheetname='data', skiprows=[0], header=None, usecols='A:D')\n",
    "    q.columns=['year','month','day','flow_cfs']\n",
    "    q_dates=pd.to_datetime(q.loc[:,['year','month','day']])\n",
    "    q.set_index(q_dates, inplace=True)\n",
    "    q.drop(['year','month','day'],axis=1, inplace=True)\n",
    "    q_cms=q.flow_cfs/(3.28084**3)\n",
    "    q_mmday=q_cms*1000*3600*24/drainage_area\n",
    "    q=pd.concat([q_cms, q, q_mmday],axis=1)\n",
    "    q.columns=['flow_cms','flow_cfs', 'flow_mmday']\n",
    "    return q\n",
    "\n",
    "def import_obs_folders(obs_folder,streamflow_obs_input_txt):\n",
    "    os.chdir(obs_folder)\n",
    "    LM_usgs= np.genfromtxt(streamflow_obs_input_txt,skip_header=1,dtype=str)                   \n",
    "    n_1=len(LM_usgs[:,0]) # n is number of days in the record\n",
    "    date_LM=np.full(n_1,'', dtype=object) # Preallocate date_1 matrix\n",
    "    for x in range(0,n_1): # Cycle through all days of the year\n",
    "        date_LM_temp=datetime.date(int(LM_usgs[x,0]),int(LM_usgs[x,1]),\n",
    "                                               int(LM_usgs[x,2]))\n",
    "        # make numpy array of individual temporary datetime objects\n",
    "        date_LM[x]=date_LM_temp # enter temporary object into preallocated date matrix\n",
    "    del(date_LM_temp) # delete temporay object\n",
    "    \n",
    "    # Extract remaining variables and convert to standard units:\n",
    "    Q_day=np.array((LM_usgs[:,3]), dtype='float64')\n",
    "    Q_day=Q_day/(3.28084**3) # convert from ft^3/s to m^3/s\n",
    "    SSC_day=np.array(LM_usgs[:,7], dtype='float64')\n",
    "    SSL_day=np.array(LM_usgs[:,9], dtype='float64')\n",
    "    T_day=np.array(LM_usgs[:,11], dtype='float64')\n",
    "    \n",
    "    LM_data=pd.DataFrame({\"Q_m3s\": Q_day, \"SSC_mgL\": SSC_day,\n",
    "                          \"SSL_tonsday\":SSL_day, \"T_fnu\":T_day},index=pd.to_datetime(date_LM))\n",
    "    \n",
    "    # Now find the 50 % exceedance flow at Lakem Mills gage- use complete water years only\n",
    "    Q_curve=LM_data.loc[datetime.date(1994, 10, 1):datetime.date(1997, 9, 30), 'Q_m3s'].values\n",
    "    Q_curve=np.append(Q_curve,LM_data.loc[datetime.date(2004, 10, 1):datetime.date(2011, 9, 30), 'Q_m3s'].values)\n",
    "    \n",
    "    # FUTURE:  Use calibrated DHSVM outputs instead?\n",
    "    Q_RC=np.sort(Q_curve)\n",
    "    n_curve=len(Q_curve)\n",
    "    ep_Q_curve=np.array(range(1,n_curve+1))/(n_curve+1)\n",
    "    cum_pcntl=ep_Q_curve\n",
    "    Q_LM_50EP=Q_RC[np.where(cum_pcntl==find_nearest(cum_pcntl, 0.50))] # 50th percentile\n",
    "            \n",
    "    return (LM_data, Q_LM_50EP)\n",
    "\n",
    "def compute_NSE_rs (modeled, observed):\n",
    "    NSE=1-((np.sum((modeled-observed)**2))/(np.sum((modeled-np.mean(observed))**2)))\n",
    "    WC,WC_0,WC_r, WC_p, WC_err=scipy.stats.linregress(modeled, observed)\n",
    "    r2=WC_r**2\n",
    "    print('r2=',r2)\n",
    "    print('NSE=',NSE)\n",
    "    return NSE, r2\n",
    "\n",
    "\n",
    "def setup_network(model_input_folder):                      \n",
    "    #os.chdir(model_input_folder)  \n",
    "    network=pd.read_table(network_input, delimiter='\\t', index_col=0,\\\n",
    "                          usecols=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "    network.columns=['segment_length_m','local_ca','dest_channel_id',\\\n",
    "                     'segment_slope','total_ca_mean','segment_order',\\\n",
    "                     'channel_class_id'] \n",
    "    return (network)\n",
    "\n",
    "#network=setup_network(model_input_folder)\n",
    "\n",
    "def run_stochastic_mass_wasting(ref_stream, a_u_ref, b_u_ref, a_d_ref, b_d_ref, a_n, b_n, ng_obs_bar, S, total_ca_ref, total_ca, Qref, Q):  \n",
    "    rho_w=1000\n",
    "    # ref stream values for given flow\n",
    "    Uref=a_u_ref*Qref**b_u_ref\n",
    "    Dref=a_d_ref*Qref**b_d_ref\n",
    "    # stream-of-interest values for given flow\n",
    "    U=Uref*(total_ca**exp_w_us)/(total_ca_ref**exp_w_us)\n",
    "    D=Dref*(total_ca**exp_d_us)/(total_ca_ref**exp_d_us)\n",
    "    ng=ng_obs_bar*a_n*D**b_n\n",
    "    tau=rho_w*g*((ng*U)**(3/2))*S**(1/4)\n",
    "    u=(tau/rho_w)**0.5\n",
    "    return tau, u         \n",
    "\n",
    "def compute_channel_properties(ref_stream, a_u_ref, b_u_ref, a_d_ref, b_d_ref, a_n, b_n, ng_obs_bar, S, total_ca_ref, total_ca, Qref, Q):  \n",
    "    rho_w=1000\n",
    "    # ref stream values for given flow\n",
    "    Uref=a_u_ref*Qref**b_u_ref\n",
    "    Dref=a_d_ref*Qref**b_d_ref\n",
    "    # stream-of-interest values for given flow\n",
    "    U=Uref*(total_ca**exp_u_us)/(total_ca_ref**exp_u_us)\n",
    "    D=Dref*(total_ca**exp_d_us)/(total_ca_ref**exp_d_us)\n",
    "    ng=ng_obs_bar*a_n*D**b_n\n",
    "    tau=rho_w*g*((ng*U)**(3/2))*S**(1/4)\n",
    "    u_star=(tau/rho_w)**0.5\n",
    "    return tau, u_star         \n",
    "        \n",
    "def run_wc2003_2F_model (tau, tau_r_sand,tau_r_gravel):\n",
    "    # Constants\n",
    "    A=14\n",
    "    chi=0.894\n",
    "    exp=0.5\n",
    "    phi_prime=1.35 \n",
    "             \n",
    "    # Run WC 2003 Two-Fraction Model\n",
    "    phi_gravel=tau/tau_r_gravel\n",
    "    phi_sand=tau/tau_r_sand\n",
    "    \n",
    "    if phi_gravel<phi_prime:\n",
    "        Wstar_gravel=0.002*(phi_gravel)**7.5\n",
    "    elif chi/((phi_gravel)**exp)>=1: # Checka that term in paraentheses is not negative\n",
    "       Wstar_gravel=0\n",
    "    else:\n",
    "        Wstar_gravel=A*((1-(chi/((phi_gravel)**exp)))**(4.5))\n",
    "\n",
    "    if phi_sand<phi_prime:\n",
    "        Wstar_sand=0.002*(phi_sand)**7.5\n",
    "    elif chi/((phi_sand)**exp)>=1: # Checka that term in paraentheses is not negative\n",
    "       Wstar_sand=0\n",
    "    else:\n",
    "        Wstar_sand=A*((1-(chi/((phi_sand)**exp)))**(4.5))        \n",
    "    return (Wstar_gravel, Wstar_sand)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create arrays of streamflow forcings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set sediment model run time = length of modeled streamflow input file output.Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_start_date=datetime.date(2000,1,1)\n",
    "forcing_end_date=datetime.date(2009,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Outflow.Only' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-36426664407b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreamflow_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstreamflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforcing_date_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Outflow.Only' does not exist"
     ]
    }
   ],
   "source": [
    "#List of all links in network for running sediment model\n",
    "stream_columns=['Date',1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439]\n",
    "\n",
    "forcing_date_range=pd.date_range(forcing_start_date, forcing_end_date)\n",
    "streamflow=pd.DataFrame(index=forcing_date_range, columns=stream_columns)\n",
    "chunksize = 8\n",
    "i=0\n",
    "for chunk in pd.read_table(streamflow_input, skiprows=[0,1], header=None,\\\n",
    "                               sep='\\s+', chunksize=chunksize):\n",
    "    chunk.columns=stream_columns\n",
    "    streamflow.loc[forcing_date_range[i],:]=chunk.sum(axis=0)[1::]/8\n",
    "    i=i+1\n",
    "last_day=pd.read_table(streamflow_input, skiprows=np.arange(0,(i-1)*8+2), \n",
    "                           header=None,sep='\\s+')\n",
    "last_day.columns=stream_columns\n",
    "streamflow.loc[forcing_date_range[i-1],:]=last_day.sum(axis=0)[1::]/8\n",
    "Qmod_median=streamflow.median(axis=0)\n",
    "pickle.dump(streamflow, open(\"streamflow.py\", \"wb\"))\n",
    "pickle.dump(Qmod_median, open(\"Qmod_median.py\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to run sediment in Headwaters Suiattle HUC12 basin only  - extracted from ArcGIS map SaukSediment.mxd \n",
    "\n",
    "headstream_columns=[260,270,276,298,300,301,316,317,318,323,325,329,333,339,340,341,342,345,349,351,352,368,371]\n",
    "headstream=streamflow.loc[:,headstream_columns]\n",
    "Qmod_median=headstream.median(axis=0)\n",
    "pickle.dump(headstream, open(\"streamflow.json\", \"wb\"))\n",
    "pickle.dump(Qmod_median, open(\"Qmod_median.json\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_stream=260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Create sediment model network file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=setup_network(homedir)\n",
    "# Downstream links and distances\n",
    "ds=pd.DataFrame(index=network.index, columns=['ds_strms','ds_dist_array','ds_dist_m']) # establish array of downstream stream numbers\n",
    "strm_orders_rev=np.unique(network.segment_order)[::-1] # get all segement orders in the watershed and sort from highest to lowest\n",
    "for st_o in strm_orders_rev:\n",
    "    for i in network.index:\n",
    "        if network.segment_order.loc[i]==st_o: # go in order of stream orders\n",
    "            j=network.loc[i, 'dest_channel_id']\n",
    "            if j in network.index:\n",
    "                ds.loc[i,'ds_strms']=np.append(j, ds.loc[j,'ds_strms'])\n",
    "                ds.loc[i,'ds_dist_array']=np.append(network.loc[j, 'segment_length_m'], ds.loc[j,'ds_strms'])\n",
    "                ds.loc[i,'ds_dist_m']=np.nansum(ds.loc[i,'ds_dist_array'])\n",
    "            else:\n",
    "                ds.loc[i,'ds_strms']=[]\n",
    "                ds.loc[i,'ds_dist_array']=[]\n",
    "                ds.loc[i,'ds_dist_m']=0\n",
    "                \n",
    "network=pd.concat([network, ds], axis=1, join_axes=[network.index]) # add these to network array \n",
    "network[['ds_dist_m']]=network[['ds_dist_m']].astype(float)\n",
    "network = network.loc[:,~network.columns.duplicated()] # Ensure that there are no duplicate columns!\n",
    "    \n",
    "# Stream Width- assume constant\n",
    "width=pd.DataFrame(index=network.index, columns=['width'],\n",
    "                       data=w_const_ref*((network['total_ca_mean'].values)**exp_w_us)/(((network['total_ca_mean'][ref_stream])**exp_w_us)))\n",
    "network=pd.concat([network, width],axis=1, join_axes=[network.index]) # add these to network array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use for full network\n",
    "#strm_link_vals=network.index\n",
    "\n",
    "#Use for smaller network\n",
    "strm_link_vals=headstream_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Qmod_median[ref_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain Size\n",
    "#os.chdir(model_data_folder)\n",
    "# strm_link_vals=pickle.load(open('strm_link_vals.py', 'rb')) # stream link values of interest\n",
    "n_strms=len(strm_link_vals) # number of streams/tributary areas\n",
    "strm_orders=np.unique(network.segment_order)\n",
    "    \n",
    "## Upstream grain size diameters from Downstream Fining Equation\n",
    "d90_ch_us=d90ch_LM/np.exp(-abrasion_alpha*(network.ds_dist_m-network.ds_dist_m[ref_stream])/1000)\n",
    "dsand_ch_us=dsandch_LM/np.exp(-abrasion_alpha*(network.ds_dist_m-network.ds_dist_m[ref_stream])/1000)\n",
    "dgravel_ch_us=dgravelch_LM/np.exp(-abrasion_alpha*(network.ds_dist_m-network.ds_dist_m[ref_stream])/1000)\n",
    "dmean_ch_us=dmeanch_LM/np.exp(-abrasion_alpha*(network.ds_dist_m-network.ds_dist_m[ref_stream])/1000)\n",
    "\n",
    "Qmod_median=pickle.load(open('Qmod_median.json', 'rb'))\n",
    "H_median_lm=a_d_ref*(Qmod_median[ref_stream])**b_d_ref   \n",
    "H_median_all=H_median_lm*((network['total_ca_mean'].values)**exp_d_us)/(((network['total_ca_mean'][ref_stream])**exp_d_us))\n",
    "ng_bar=(1/(np.sqrt(8*g)))*(H_median_all**(1/6))/(1.26-2.16*np.log10(d90_ch_us/H_median_all))\n",
    "    \n",
    "network=pd.concat([network,pd.DataFrame({\"d90_ch_m\": d90_ch_us,\n",
    "                                          \"dsand_ch_m\": dsand_ch_us,\n",
    "                                          \"dgravel_ch_m\": dgravel_ch_us,\n",
    "                                          \"dmean_ch_m\": dmean_ch_us,\n",
    "                                          \"Ch_Fs_strms\": Ch_Fs_LM, # start out with all the same as LM and will let evolve\n",
    "                                          \"ng_bar\":ng_bar},index=strm_link_vals)], axis=1, join_axes=[network.index]) \n",
    "    \n",
    "network=network.loc[strm_link_vals,:]\n",
    "del(dsand_ch_us, dgravel_ch_us, dmean_ch_us)\n",
    "    #os.chdir(model_data_folder)\n",
    "pickle.dump(network, open(\"network.json\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Initialize model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strm_link_vals=network.index # stream link values of interest\n",
    "n_strms=len(strm_link_vals) # number of streams/tributary areas\n",
    "strm_orders=np.unique(network.segment_order) # array of unique stream orders\n",
    "start_year_len=len(streamflow.index[streamflow.index.year==streamflow.index[0].year]) # number of days in first year\n",
    "\n",
    "# Preallocate sediment volume data frames representing sediment \"buckets\" at the end of each timestep (day)\n",
    "# Vg=gravel; Vs=sand; Vm=mud (silt, clay)\n",
    "# Volumes stored on bed [m3]\n",
    "Vg_b=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vs_b=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vm_b=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "\n",
    "# Volumes deposited from mass wasting event [m3]\n",
    "Vg_mw=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vs_mw=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vm_mw=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "\n",
    "# Volume of bedload that stream had capacity to transport  [m3]\n",
    "Vg_cap=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vs_cap=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "\n",
    "# Volume transported out of stream  [m3]\n",
    "Vg_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vs_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vsb_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vss_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vm_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "\n",
    "# Preallocate sediment volume data frames representing sediment \"buckets\" at the end of each year\n",
    "# Volumes stored on bed [m3]\n",
    "Vg_b_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vs_b_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vm_b_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "# Volumes deposited from mass wasting event [m3]\n",
    "Vg_mw_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vs_mw_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vm_mw_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "# Volume of bedload  that stream had capacity to transport  [m3]\n",
    "Vg_cap_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year)) \n",
    "Vs_cap_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "# Volume transported out of stream  [m3]\n",
    "Vg_t_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vs_t_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vsb_t_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vss_t_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "Vm_t_annual=pd.DataFrame(0, index=strm_link_vals, columns=np.unique(streamflow.index.year))\n",
    "\n",
    "# Other- temporary\n",
    "C_ss=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "Vs_dep=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,start_year_len+1))\n",
    "\n",
    "#Set any initial (time=0) conditions in streams\n",
    "# Volumes stored on bed [m3]\n",
    "Vg_b.loc[:,0]=0.33*init_depth*network.segment_length_m*network.width # set initial volume of sediment stored in channel\n",
    "Vs_b.loc[:,0]=0.34*init_depth*network.segment_length_m*network.width # set initial volume of sediment stored in channel\n",
    "Vg_b.loc[:,0]=0.33*init_depth*network.segment_length_m*network.width # set initial volume of sediment stored in channel\n",
    "# Volumes deposited from mass wasting event [m3]\n",
    "Vg_mw.loc[:,0]=0\n",
    "Vs_mw.loc[:,0]=0\n",
    "Vm_mw.loc[:,0]=0\n",
    "\n",
    "# Volume that stream had capacity to transport  [m3]\n",
    "Vg_cap.loc[:,0]=0\n",
    "Vs_cap.loc[:,0]=0\n",
    "\n",
    "# Volume transported out of stream  [m3]\n",
    "Vg_t.loc[:,0]=0\n",
    "Vs_t.loc[:,0]=0\n",
    "Vm_t.loc[:,0]=0\n",
    "\n",
    "# Set arrays for times between mass wasting events\n",
    "time_mw=pd.DataFrame(0, index=strm_link_vals, columns=['output']) # time since last mass wasting event for each stream link\n",
    "tL_mw=np.ceil(np.random.exponential(scale=beta_mw, size=len(strm_link_vals))) # time (years) until first mass wasting event- randomly sampled from exponential distribution\n",
    "tL_mw=pd.DataFrame(data=tL_mw, index=strm_link_vals, columns=['output']) # turn into a data frame that will grow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Run Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "date_string=time.strftime(\"%Y-%m-%d_%H%M\")\n",
    "os.mkdir(date_string)\n",
    "day_of_year=0\n",
    "## RUN MODEL\n",
    "for i in range (0,len(streamflow.index)):\n",
    "    print(streamflow.index[i])\n",
    "    date=streamflow.index[i]\n",
    "    day_of_year=day_of_year+1\n",
    "    time_mw=time_mw+1 # start new year\n",
    "   # run stochastic mass wasting generator for all stream links\n",
    "    for st_o in strm_orders:\n",
    "        for j in network.index.values:\n",
    "            if network.segment_order.loc[j]==st_o: # go in order of stream orders\n",
    "                if time_mw.loc[j,'output']==tL_mw.loc[j,'output']: # if time since the last mass wasting event is the same as randomly generated lag time\n",
    "                    # Generate random values for percent gravel, sand, and mud,\n",
    "                    temp_rand=np.random.rand(3)\n",
    "                    mw_pcnts=temp_rand/sum(temp_rand)\n",
    "                    # Generate mass wasting volumes based on denudation rate\n",
    "                    Vg_mw.loc[j,day_of_year]=mw_pcnts[0]*Drate*network.loc[j,'local_ca']*(1/1000)*tL_mw.loc[j,'output']\n",
    "                    Vs_mw.loc[j,day_of_year]=mw_pcnts[1]*Drate*network.loc[j,'local_ca']*(1/1000)*tL_mw.loc[j,'output']                   \n",
    "                    Vm_mw.loc[j,day_of_year]=mw_pcnts[2]*Drate*network.loc[j,'local_ca']*(1/1000)*tL_mw.loc[j,'output']                   \n",
    "                    # Update mass wasting clocks\n",
    "                    time_mw.loc[j,'output']=0 # Reset counter until next mass wasting event\n",
    "                    tL_mw.loc[j,'output']=np.ceil(np.random.exponential(scale=beta_mw))  # generate a new lag time until next mass wasting event and add it to array of lag times#\n",
    "                else: # If have not reach the lag time for a mass wasting event at the stream then the volume will be the same as last year\n",
    "                    # Not mass wasting volume is added\n",
    "                    Vg_mw.loc[j,day_of_year]=0\n",
    "                    Vs_mw.loc[j,day_of_year]=0\n",
    "                    Vm_mw.loc[j,day_of_year]=0\n",
    "\n",
    "                # Add mass wasting volume to the bed from previous timestep\n",
    "                Vg_b.loc[j,day_of_year]=Vg_b.loc[j,day_of_year-1]+Vg_mw.loc[j,day_of_year]\n",
    "                Vs_b.loc[j,day_of_year]=Vs_b.loc[j,day_of_year-1]+Vs_mw.loc[j,day_of_year] \n",
    "                Vm_b.loc[j,day_of_year]=Vm_b.loc[j,day_of_year-1]+Vm_mw.loc[j,day_of_year] \n",
    "\n",
    "                # Add in sediment from upstream tributaries\n",
    "                if st_o!=1: # if not at a headwater stream (i.e., stream order >1), need to add volume of sediment coming from upstream\n",
    "                    feeder_links=network.loc[network['dest_channel_id']==j].index.values # find the stream links that feed into the link which are immediately upstream\n",
    "                    # sum up the volume transported out of the upstream link and add to current volume\n",
    "                    Vg_b.loc[j,day_of_year]=Vg_b.loc[j,day_of_year]+np.nansum(Vg_t.loc[feeder_links,day_of_year])  \n",
    "                    Vs_b.loc[j,day_of_year]=Vs_b.loc[j,day_of_year]+np.nansum(Vs_t.loc[feeder_links,day_of_year])\n",
    "                    Vm_b.loc[j,day_of_year]=Vm_b.loc[j,day_of_year]+np.nansum(Vm_t.loc[feeder_links,day_of_year])\n",
    "                \n",
    "                # Compute shear stress and shear velocity from modeled flow\n",
    "                Q=streamflow.loc[date,j]\n",
    "                Qvol=Q*3600*24 # m3 of streamflow throughout the day [m3]\n",
    "                tau_strm, u_strm= compute_channel_properties(ref_stream, a_u_ref, b_u_ref, a_d_ref, b_d_ref, a_n, b_n, network['ng_bar'][j], \n",
    "                                                             network.segment_slope[j], network['total_ca_mean'][ref_stream], network['total_ca_mean'][j], \n",
    "                                                             streamflow.loc[date,ref_stream], Q)\n",
    "\n",
    "                # Suspended Sediment\n",
    "                C_ss.loc[j,day_of_year]=(Vs_b.loc[j,day_of_year]+Vm_b.loc[j,day_of_year])/Qvol # Concentration of suspended sediment (unitless)\n",
    "                \n",
    "                # Suspended sediment- silt: Compute deposition and transport\n",
    "                if tau_strm>tau_c_fines:\n",
    "                    Vm_d=0\n",
    "                else: \n",
    "                    if C_ss.loc[j,day_of_year]<=c1_m:\n",
    "                        v_ss_m=a_w_m*(c1_m**n_w_m)/((c1_m**2+b_w_m**2)**m_w_m)\n",
    "                    if C_ss.loc[j,day_of_year]>c2_m:\n",
    "                        v_ss_m=a_w_m*(c2_m**n_w_m)/((c2_m**2+b_w_m**2)**m_w_m)\n",
    "                    else:\n",
    "                        v_ss_m=a_w_m*(C_ss.loc[j,day_of_year]**n_w_m)/((C_ss.loc[j,day_of_year]**2+b_w_m**2)**m_w_m)                        \n",
    "                    Vm_d=(1-(tau_strm/tau_c_fines))*v_ss_m*C_ss.loc[j,day_of_year]*network.loc[j,'segment_length_m']*network.loc[j,'width']*3600*24\n",
    "                Vm_t.loc[j,day_of_year]=Vm_b.loc[j,day_of_year]-min(Vm_d, Vm_b.loc[j,day_of_year])\n",
    "                Vm_b.loc[j,day_of_year]=max(Vm_b.loc[j,day_of_year]-Vm_t.loc[j,day_of_year],0)\n",
    "                \n",
    "                # Suspended sediment- sand: Compute deposition/erosion\n",
    "                #v_ss_s=g*(network.loc[j,'dsand_ch_m']**2)*(sg-1)/(18*v) # settling velocity of sand\n",
    "                d_star_ss=(((sg-1)*g*network.loc[j,'dsand_ch_m']**3)/(v**2))**(1/3)\n",
    "                v_ss_s=(v/network.loc[j,'dsand_ch_m'])*(np.sqrt((1/4)*(24/1.5)**(2/1)+((4*d_star_ss**3)/(3*1.5))**(1/1))-(1/2)*(24/1.5)**(1/1))**1                \n",
    "                Z_R=v_ss_s/(0.41*u_strm)\n",
    "                int_Z_R=1/(c0_ss+c1_ss*Z_R+c2_ss*Z_R**2+c3_ss*Z_R**3+c4_ss*Z_R**4+c5_ss*Z_R**5+c6_ss*Z_R**6)\n",
    "                Vs_dep_comp=v_ss_s*C_ss.loc[j,day_of_year]/int_Z_R*network.loc[j,'segment_length_m']*network.loc[j,'width']*3600*24 #m3/day- volume of deposited sand\n",
    "                if Vs_dep_comp<0:\n",
    "                    Vs_dep.loc[j,day_of_year]=0\n",
    "                elif Vs_dep_comp>Vs_b.loc[j,day_of_year]:\n",
    "                    Vs_dep.loc[j,day_of_year]=Vs_b.loc[j,day_of_year]\n",
    "                else:\n",
    "                    Vs_dep.loc[j,day_of_year]=Vs_dep_comp\n",
    "\n",
    "                # Sand transported out as suspended sediment\n",
    "                Vss_t.loc[j,day_of_year]=Vs_b.loc[j,day_of_year]-Vs_dep.loc[j,day_of_year]\n",
    "                \n",
    "                # Sand remaining on bed, with potential to be transferred as bedload\n",
    "                Vs_b.loc[j,day_of_year]=Vs_dep.loc[j,day_of_year]\n",
    "                \n",
    "                # Recompute W&C 2003 parameterizaion of sand and gravel on the channel bed\n",
    "                network.loc[j,'Ch_Fs_strms']=Vs_b.loc[j,day_of_year]/(Vs_b.loc[j,day_of_year]+Vg_b.loc[j,day_of_year])\n",
    "                tau_star_rsm=0.021+0.015*np.exp(-20*network.loc[j,'Ch_Fs_strms']) # dimensionless reference shear stress for mean grain size\n",
    "                tau_rsm=tau_star_rsm*(sg-1)*rho_w*g*(network.loc[j,'dmean_ch_m']) # reference shear stress for mean grain size [N/m2]\n",
    "                b_sand=0.67/(1+np.exp(1.5-(network.loc[j,'dsand_ch_m']/network.loc[j,'dmean_ch_m']))) # b parameter for sand\n",
    "                b_gravel=0.67/(1+np.exp(1.5-(network.loc[j,'dgravel_ch_m']/network.loc[j,'dmean_ch_m']))) # b parameter for gravel\n",
    "                tau_r_sand=tau_rsm*(network.loc[j,'dsand_ch_m']/network.loc[j,'dmean_ch_m'])**b_sand # reference tau for sand [N/m2]\n",
    "                tau_r_gravel=tau_rsm*(network.loc[j,'dgravel_ch_m']/network.loc[j,'dmean_ch_m'])**b_gravel # reference tau for gravel [N/m2]\n",
    "                tau_star_r_sand=tau_r_sand/(rho_w*g*(sg-1)*network.loc[j,'dsand_ch_m'])\n",
    "                tau_star_r_gravel=tau_r_gravel/(rho_w*g*(sg-1)*network.loc[j,'dgravel_ch_m'])\n",
    "                         \n",
    "                # Compute Bedload sediment transport capacity with calibrated Wilcock and Crowe equation\n",
    "                Wstar_gravel, Wstar_sand=run_wc2003_2F_model (tau_strm, tau_star_r_sand, tau_star_r_gravel)\n",
    "                \n",
    "                # Compute gravel transport capacity, volume transported, and volume remaining\n",
    "                Vg_cap.loc[j,day_of_year]=3600*24*((u_strm)**3)*Wstar_gravel*(1-network.loc[j,'Ch_Fs_strms'])/((sg-1)*g)*network.loc[j,'width'] #  m3 (unit is m3/day; timestep is 1 day)\n",
    "                Vg_t.loc[j,day_of_year]=np.min([Vg_cap.loc[j,day_of_year],Vg_b.loc[j,day_of_year]]) # gravel: volume transported is the minimum between available volume and capacity of flow\n",
    "                Vg_b.loc[j,day_of_year]=Vg_b.loc[j,day_of_year]-Vg_t.loc[j,day_of_year] # new volume deposited in stream- difference between existing volume and volume transported\n",
    "\n",
    "                # Compute sand bedload transport capacity, volume transported, and volume remaining\n",
    "                Vs_cap.loc[j,day_of_year]=3600*24*((u_strm)**3)*Wstar_sand*(network.loc[j,'Ch_Fs_strms'])/((sg-1)*g)*network.loc[j,'width'] # m3 (unit is m3/day; timestep is 1 day)\n",
    "                Vsb_t.loc[j,day_of_year]=np.min([Vs_cap.loc[j,day_of_year],Vs_b.loc[j,day_of_year]]) # sand: volume transported is the minimum between available volume and capacity of flow\n",
    "                Vs_t.loc[j,day_of_year]=Vsb_t.loc[j,day_of_year]+Vss_t.loc[j,day_of_year] # add volumes of sand transported as bedload and suspended load \n",
    "                Vs_b.loc[j,day_of_year]=Vs_b.loc[j,day_of_year]-Vsb_t.loc[j,day_of_year] # new volume deposited in stream- difference between existing volume and volume transported                             \n",
    "                \n",
    "    if date.month==12 and date.day==31:\n",
    "        if date.year==streamflow.index[streamflow.index.year==streamflow.index[-1].year][0].year:\n",
    "            next_year_len=0\n",
    "        else: next_year_len=len(streamflow.index[streamflow.index.year==date.year+1])\n",
    "        \n",
    "        Vg_b_last=Vg_b.loc[:, Vg_b.columns[-1]]\n",
    "        Vs_b_last=Vs_b.loc[:, Vs_b.columns[-1]]\n",
    "        Vm_b_last=Vm_b.loc[:, Vm_b.columns[-1]]\n",
    "        Vg_mw_last=Vg_mw.loc[:, Vg_mw.columns[-1]]\n",
    "        Vs_mw_last=Vs_mw.loc[:, Vs_mw.columns[-1]]\n",
    "        Vm_mw_last=Vm_mw.loc[:, Vm_mw.columns[-1]]\n",
    "        Vg_cap_last=Vg_cap.loc[:, Vg_cap.columns[-1]]\n",
    "        Vs_cap_last=Vs_cap.loc[:, Vs_cap.columns[-1]]\n",
    "        Vg_t_last=Vg_t.loc[:, Vg_t.columns[-1]]\n",
    "        Vs_t_last=Vs_t.loc[:, Vs_t.columns[-1]]\n",
    "        Vsb_t_last=Vsb_t.loc[:, Vsb_t.columns[-1]]\n",
    "        Vss_t_last=Vss_t.loc[:, Vss_t.columns[-1]]\n",
    "        Vm_t_last=Vm_t.loc[:, Vm_t.columns[-1]]\n",
    "        C_ss_last=C_ss.loc[:, C_ss.columns[-1]]\n",
    "        Vs_dep_last=Vs_dep.loc[:, C_ss.columns[-1]]\n",
    "\n",
    "        Vg_b_annual.loc[:, date.year]=Vg_b_last\n",
    "        Vs_b_annual.loc[:, date.year]=Vs_b_last\n",
    "        Vg_mw_annual.loc[:, date.year]=Vg_mw[1::].sum(axis=1)\n",
    "        Vs_mw_annual.loc[:, date.year]=Vs_mw[1::].sum(axis=1)\n",
    "        Vg_cap_annual.loc[:, date.year]=Vg_cap[1::].sum(axis=1)\n",
    "        Vs_cap_annual.loc[:, date.year]=Vs_cap[1::].sum(axis=1)\n",
    "        Vg_t_annual.loc[:, date.year]=Vg_t[1::].sum(axis=1)\n",
    "        Vs_t_annual.loc[:, date.year]=Vs_t[1::].sum(axis=1)\n",
    "        Vsb_t_annual.loc[:, date.year]=Vsb_t[1::].sum(axis=1)\n",
    "        Vss_t_annual.loc[:, date.year]=Vss_t[1::].sum(axis=1)\n",
    "        Vm_t_annual.loc[:, date.year]=Vm_t[1::].sum(axis=1)\n",
    "        \n",
    "        np.save('Vg_b_annual', Vg_b_annual)\n",
    "        np.save('Vs_b_annual', Vs_b_annual)\n",
    "        np.save('Vg_mw_annual', Vg_mw_annual)   \n",
    "        np.save('Vs_mw_annual', Vs_mw_annual)   \n",
    "        np.save('Vg_cap_annual', Vg_cap_annual)\n",
    "        np.save('Vs_cap_annual', Vs_cap_annual)\n",
    "        np.save('Vg_t_annual', Vg_t_annual)\n",
    "        np.save('Vs_t_annual', Vs_t_annual)\n",
    "        np.save('Vsb_t_annual', Vsb_t_annual)\n",
    "        np.save('Vss_t_annual', Vss_t_annual)\n",
    "        np.save('Vm_t_annual', Vm_t_annual)\n",
    "        np.save('time_mw', time_mw)\n",
    "        np.save('tL_mw', tL_mw)\n",
    "                \n",
    "        Vg_b=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vs_b=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vg_mw=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vs_mw=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vg_cap=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vs_cap=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vg_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vs_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vsb_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vss_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        Vm_t=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1))\n",
    "        C_ss=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1)) \n",
    "        Vs_dep=pd.DataFrame(0, index=strm_link_vals, columns=np.arange(0,next_year_len+1)) \n",
    "\n",
    "        Vg_b.loc[:, Vg_b.columns[0]]=Vg_b_last\n",
    "        Vs_b.loc[:, Vs_b.columns[0]]=Vs_b_last\n",
    "        Vg_mw.loc[:, Vg_mw.columns[0]]=Vg_mw_last\n",
    "        Vs_mw.loc[:, Vs_mw.columns[0]]=Vs_mw_last\n",
    "        Vg_cap.loc[:, Vg_cap.columns[0]]=Vg_cap_last\n",
    "        Vs_cap.loc[:, Vs_cap.columns[0]]=Vs_cap_last\n",
    "        Vg_t.loc[:, Vg_t.columns[0]]=Vg_t_last\n",
    "        Vs_t.loc[:, Vs_t.columns[0]]=Vs_t_last\n",
    "        Vsb_t.loc[:, Vsb_t.columns[0]]=Vsb_t_last\n",
    "        Vss_t.loc[:, Vss_t.columns[0]]=Vss_t_last\n",
    "        Vm_t.loc[:, Vm_t.columns[0]]=Vm_t_last\n",
    "        C_ss.loc[:, C_ss.columns[0]]=C_ss_last\n",
    "        Vs_dep.loc[:, Vs_dep.columns[0]]=Vs_dep_last\n",
    "        \n",
    "        day_of_year=0\n",
    "\n",
    "\n",
    "# Save results!\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "np.save('Vg_b_annual', Vg_b_annual)\n",
    "np.save('Vs_b_annual', Vs_b_annual)\n",
    "np.save('Vg_mw_annual', Vg_mw_annual)\n",
    "np.save('Vs_mw_annual', Vs_mw_annual)\n",
    "np.save('Vg_cap_annual', Vg_cap_annual)\n",
    "np.save('Vs_cap_annual', Vs_cap_annual)\n",
    "np.save('Vg_t_annual', Vg_t_annual)\n",
    "np.save('Vs_t_annual', Vs_t_annual)\n",
    "np.save('Vsb_t_annual', Vsb_t_annual)\n",
    "np.save('Vss_t_annual', Vss_t_annual)\n",
    "np.save('Vm_t_annual', Vm_t_annual)\n",
    "np.save('Vs_dep', Vs_dep)\n",
    "np.save('C_ss',  C_ss)\n",
    "np.save('time_mw', time_mw)\n",
    "np.save('tL_mw', tL_mw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Save model results into a text file at the end of each model run \n",
    "\n",
    "If you want to explore the hydrographs created in this application in another software environment, you will run the code below to create a single .csv file that contains modeled hydrograps at each location in the watershed. Please run this code for each run you completed separately. The .csv file can be used in Excel, Matlab or any other software. The name of the new file will be printed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_flag='Sauk'\n",
    "time_flag='2000'\n",
    "routing_method='wc'\n",
    "\n",
    "network_output = os.path.join(homedir, date_string)\n",
    "os.chdir(network_output) \n",
    "\n",
    "Vg_b_annual=pd.DataFrame()\n",
    "Vg_b_annual.to_csv(str(basin_flag + '_' + time_flag + '_' + routing_method  + '_Vg_b_annual.csv'),index=False,header=None,sep=',')\n",
    "\n",
    "time_mw=pd.DataFrame()\n",
    "time_mw.to_csv(str(basin_flag + '_' + time_flag + '_' + routing_method  + 'time_mw.csv'),index=False,header=None,sep=',')\n",
    "\n",
    "tL_mw=pd.DataFrame()\n",
    "tL_mw.to_csv(str(basin_flag + '_' + time_flag + '_' + routing_method  + 'tL_mw.csv'),index=False,header=None,sep=',')\n",
    "\n",
    "C_ss=pd.DataFrame()\n",
    "C_ss.to_csv(str(basin_flag + '_' + time_flag + '_' + routing_method  + 'C_ss.csv'),index=False,header=None,sep=',')\n",
    "os.chdir(homedir) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tL_mw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Analyze results at different locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /> <img src=\"https://www.hydroshare.org/resource/d5f1f96849554e03aa98bed5aff36cf7/data/contents/Sauk-SuiattleHeadwaters_Map.JPG\" style=\"float:center;width:600px;padding:40px\">\n",
    "\n",
    "\n",
    "Figure 2. Upper Suiattle River glaciated streamflow network locations\n",
    "\n",
    "\n",
    "Table 1a. Sediment Transport Model Results at Glaciated streams  \n",
    "\n",
    "| Glacier name        | USGS Code           |  Drainage Area (sq km)  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Dusty     | Headwater Suiattle River (HUC 171100060201) | 6.8 |\n",
    "| Chocolate     | Headwater Suiattle River (HUC 171100060201) | 3.8 |\n",
    "| Cool     | Headwater Suiattle River (HUC 171100060201) | 1.5 |\n",
    "| Suiattle     | Headwater Suiattle River (HUC 171100060201) | 5.7 |\n",
    "| Honeycomb    | Headwater Suiattle River (HUC 171100060201) | 2.7 |\n",
    "| Vista    | Miners Creek Suiattle River (HUC 171100060202)  | 4.7 |\n",
    "| Ermine    | Miners Creek Suiattle River (HUC 171100060202)  | 3.3 |\n",
    "\n",
    "Table 1b. Sediment Transport Model Results at Streamflow Locations in the lower elevation network of the Sauk Watershed with recent streamflow observations\n",
    "\n",
    "\n",
    "| Location        | USGS Code           | Drainage Area (sq mi)  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Sauk River near Sauk, WA     | 12189500    | 714 |\n",
    "| Sauk River at Darrington, WA    | 12187500    |   293 |\n",
    "| White Chuck River at Darrington, WA    | 12186500    |   78 |\n",
    "| Sauk River above White Chuck River near Darrington, WA | 12186000    |    152 |\n",
    "\n",
    "Table 1c. Other Sediment Transport Results at Streamflow Locations in the lower elevation network of the Sauk Watershed\n",
    "\n",
    "| Location        | USGS Code           | Historic Data  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Sauk River above Clear Creek near Darrington, WA | 12187000    |  1910-1913 |\n",
    "| North Fork Sauk River near Barlow Pass, WA| 12185000    |   1917-1920 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_to_plot=[295,317,329,349,363,368,371]\n",
    "stream_name=['VistaGlacier','DustyGlacier','ChocolateGlacier','CoolGlacier','12185000','SuiattleGlacier','HoneycombGlacier']\n",
    "evaluation_years_plot=np.arange(2000,2003) #np.unique(streamflow.index.year)\n",
    "i=0\n",
    "for stream_index in streams_to_plot:  \n",
    "    i=i+1\n",
    "    plot_title=''\n",
    "    fig, (ax1, ax2, ax3, ax4)=plt.subplots(4,1, sharex=True, sharey=False)        \n",
    "    \n",
    "    ax1.plot(evaluation_years_plot,\n",
    "             Vg_mw_annual.loc[stream_index,evaluation_years_plot]/(network.loc[stream_index,'width']*network.loc[stream_index,'segment_length_m']),\n",
    "             'b-', linewidth=3, label='gravel')\n",
    "    ax1.plot(evaluation_years_plot,\n",
    "             Vs_mw_annual.loc[stream_index,evaluation_years_plot]/(network.loc[stream_index,'width']*network.loc[stream_index,'segment_length_m']),\n",
    "             'r--', linewidth=3, label='sand')\n",
    "    ax1.set_title('Stream Number '+str(stream_index)+' '+ str(stream_name[i])+'\\nSediment Deposited from Landslide',fontsize=14)\n",
    "    ax1.set_ylabel('Depth (m)',fontsize=12)\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(evaluation_years_plot, Vg_cap_annual.loc[stream_index,evaluation_years_plot],'b-', linewidth=3)  \n",
    "    ax2.plot(evaluation_years_plot, Vs_cap_annual.loc[stream_index,evaluation_years_plot],'r--', linewidth=3)  \n",
    "    ax2.set_title('Bedload Transport Capacity of Stream',fontsize=14)\n",
    "    ax2.set_ylabel('Depth (m)',fontsize=12)\n",
    "    #ax2.set_ylabel('Sediment\\nDischarge\\n(m3/s)',fontsize=12)\n",
    "    \n",
    "    ax3.plot(evaluation_years_plot,Vg_t_annual.loc[stream_index,evaluation_years_plot], 'b-',linewidth=3)\n",
    "    ax3.plot(evaluation_years_plot,Vs_t_annual.loc[stream_index,evaluation_years_plot], 'r--',linewidth=3)\n",
    "    ax3.set_title('Sediment Transported Out of Channel',fontsize=14)\n",
    "    ax3.set_ylabel('Depth (m)',fontsize=12)\n",
    "    #ax3.set_ylabel('Sediment\\nDischarge\\n(m3/s)',fontsize=12)\n",
    "    \n",
    "    ax4.plot(evaluation_years_plot,\n",
    "             Vg_b_annual.loc[stream_index,evaluation_years_plot]/(network.loc[stream_index,'width']*network.loc[stream_index,'segment_length_m']),\n",
    "             'b-', linewidth=3)\n",
    "    ax4.plot(evaluation_years_plot,\n",
    "             Vs_b_annual.loc[stream_index,evaluation_years_plot]/(network.loc[stream_index,'width']*network.loc[stream_index,'segment_length_m']),\n",
    "             'r--', linewidth=3)\n",
    "    ax4.set_title('Sediment Accumulated on Channel Bed',fontsize=14)\n",
    "    ax4.set_ylabel('Depth (m)',fontsize=12)\n",
    "    ax4.set_xlabel('Year',fontsize=16)\n",
    "    \n",
    "    sum_transported_gravel=np.sum(Vg_t_annual.loc[stream_index,evaluation_years_plot])\n",
    "    sum_transported_sand=np.sum(Vs_t_annual.loc[stream_index,evaluation_years_plot])\n",
    "    print('Stream Number', stream_index, ', Total gravel transported (10^6 m3)=',sum_transported_gravel/10**6)\n",
    "    print('Stream Number', stream_index, ', Total sand transported(10^6 m3)=',sum_transported_sand/10**6)\n",
    "    print('Stream Number', stream_index, ', Total sediment transported (10^6 m3)=', (sum_transported_gravel+sum_transported_sand)/10**6)\n",
    "    if stream_index==ref_stream:\n",
    "        sum_dep_gravel=np.sum(Vg_b_annual.loc[stream_index,evaluation_years_plot[-1]])\n",
    "        sum_dep_sand=np.sum(Vs_b_annual.loc[stream_index,evaluation_years_plot[-1]])           \n",
    "        print('Stream Number', stream_index, ', Reservoir Sedimentation (10^6 m3)=', (sum_dep_gravel+sum_dep_sand+sum_transported_gravel+sum_transported_sand)*sg/(bd_coarse*10**6))\n",
    "        print('Stream Number', stream_index, ', Reservoir Percent Gravel=', (sum_dep_gravel+sum_transported_gravel)/(sum_dep_gravel+sum_dep_sand+sum_transported_gravel+sum_transported_sand))\n",
    "        print('Stream Number', stream_index, ', ReservoirPercent Sand=', (sum_dep_sand+sum_transported_sand)/(sum_dep_gravel+sum_dep_sand+sum_transported_gravel+sum_transported_sand))\n",
    "    print('--------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information, data, or work presented herein was funded in part by the Office of Energy Efficiency and Renewable Energy (EERE), U.S. Department of Energy, under Award Number DE-EE0006506 and the Hydro Research Foundation. Neither the United States Government nor any agency thereof, nor any of their employees, makes and warranty, express or implied, or assumes and legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Disclaimer like this needed for BIA funding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Christina TO DO - add save back to HydroShare end bits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
